---
title: 'Regression Models: Mtcars'
author: "Ariel Lev"
date: "24. May 2015"
output: 
  html_document:
    keep_md: true
    css: style.css
---
###Executive Summary
With a confidence level of 95%, vehicles equiped with manual transmission drive between 0.05 to 5.83 more miles per gallon than vehicle equiped with automatic transmission.

```{r, echo=F, fig.width=12, fig.height=4, message=F}
require(ggplot2)
require(datasets)
require(ggm)
require(knitr)
require(lattice)
data(mtcars)
```

###Exploratory data analyses
The data, extracted from 1974 Motor Trend magazine, comprise gasoline mileage in miles per gallon (MPG), and `r dim(mtcars)[2] - 1` aspects of automotive design and performance for `r dim(mtcars)[1]` vehicles. The regression model attemps to quantify the MPG difference between automatic and manual transmissions.
```{r mtcars, echo=F}
kable(head(mtcars,5), caption = "Table 1.|Data for Motor Trend sample of 32 automobiles")
```

As Table 1 shows, this particular sample of `r dim(mtcars)[1]` vehicles has a bias to non-U.S. OEMs: it includes 7 Mercedes, a Porsche, a Ferrari and a Maserati. Therefore we might not expect a universal prediction model to emerge. 
In addition we can see tha the dataset contains 6 continous variables: `r names(mtcars)[c(1,3:7)]` and 5 categorical variables: `r names(mtcars)[-c(1,3:7)]`

###Regression Anaylsis
In order to study the change in the response variable MPG regarding the transmission type, any linear model must take AM as a term.
```{r, echo=T}
fit0 <- lm(mpg ~ factor(am), data = mtcars)
summary(fit0)$r.squared
```
However AM can be accounted only for a little of the variance of MPG. Our basic model reports a R square of only `r round(100*summary(fit0)$r.squared, 2)`%. In addition, checking the parital correlation of MPG and AM, controlled by WT, we can see that there is hardly **any** correlation between the two. 
```{r, echo=T}
pcor(c("mpg", "am", "wt"), var(mtcars))
```
Correlation of MPG against each one of the features suggests WT as the best single predictor. There is a loose physicall argument that supports this model, as distance can be considered as work applied to a vehicle's body. The following code shows the 7 most  with MPG correlated features, given in absoulte values, sorted from biggest to smallest. A complete correlation matrix is depcit by Figure 1.
```{r, echo=T,tidy=TRUE, message=F}
sort(abs(cor(mtcars)[1,-c(1)]), decreasing = T)[1:7]
```

```{r, echo=T,tidy=TRUE, message=F}
# shifting wt by its mean in order to yield meaningfull intercept values  
fit1 <- update(fit0, . ~ . + I(wt - mean(wt)), data = mtcars)
summary(fit1)$r.squared
```
After adding WT as a term we not only get a far larger R square, but a variance analysis yields a 46.12 F-Ratio with a very small p-value suggesting a real improvement to the predcition force of our model.
```{r, echo=T}
anv <- anova(fit0,fit1)
data.frame( F = anv$"F"[2], "p-value" = anv$"Pr(>F)"[2])
```
Predciting MPG performance may be subtle to inaccuracy, when excluding (at least intutively) an important aspect such as engine fuel consumption. When adding a third regressor to the model, i was considering either engine displacment ( a general indicator of its size and power) or 1/4 mile time. By repating the above procedure I found out that 1/4 mile time is a slightly better contributer than displacement. By adding a forth regressor, the procedure looses its strength, and it is hard to improve the model any further.  
```{r, echo=F,tidy=TRUE, message=F}
# shifting qsec by its mean in order to yield meaningfull intercept values  
fit2 <- update(fit1, . ~ . + I(qsec - mean(qsec)), data = mtcars)
```
A residual Analysis reveals a mean of `r format(mean(fit2$resid), scientific=TRUE, digits = 3)` and p-value of `r shapiro.test(fit2$resid)$p.value` in Shapiro-Wilk Normality Test, indicating that the residuals are normally distributed.
```{r, echo=T,tidy=TRUE, message=F}
# shifting qsec by its mean in order to yield meaningfull intercept values  
fit2 <- update(fit1, . ~ . + I(qsec - mean(qsec)), data = mtcars)
summary(fit2)

# extracting confidence intervals
sumCoef <- summary(fit2)$coefficients
conf1 <- sumCoef[1,] + c(-1,1)*qt(.975,fit2$df)*sumCoef[,2]
ci <- data.frame(low = c(), high = c())
for (i in 1:4) {
  ci <- rbind(ci, round(c( sumCoef[i,1], sumCoef[i,1] + c(-1,1)*qt(.975,fit2$df)*sumCoef[i,2]), 2))
  }
names(ci) <- c("coeff", "low", "high")
row.names(ci) <- row.names(sumCoef)
ci
```
###Summary
Analaysis of the above coeffecients shows with a 95% confidence that:   
1. Vehicles equiped with automatic transmission, which are **avergely weighted and avergely fast on 1/4 mile time**, drive between `r abs(ci[1,2])` to `r abs(ci[1,3])` miles per gallon  
2. Same vehicles, but with manual transmissios, drives between `r abs(ci[2,2])` to `r abs(ci[2,3])` more miles per gallons  
And in addition:
3. Between `r abs(ci[3,2])` to `r abs(ci[3,3])` decrease in MPG per 1000 lb increase in weight  
4. Between `r abs(ci[4,2])` to `r abs(ci[4,3])` decrease in MPG per one second slower in 1/4 mile time 

#Appendix

```{r, echo=F, fig.width=12, fig.height=8, message=F}
levelplot(abs(cor(mtcars)))
par(mfrow = c(2,2))
plot(fit2)
```

```{r, echo=F, fig.width=12, fig.height=8, message=F}
# cross validation code
# train_sample <- mtcars[s,];test_sample <- mtcars[-s,]
# s <- sample(1:nrow(mtcars), nrow(mtcars)*0.8, replace=FALSE);train_sample <- mtcars[s,];test_sample <- mtcars[-s,]
# fit_train <- lm(mpg ~ factor(am) + I(wt - mean(wt)) + I(qsec - mean(qsec)), data = train_sample)
# t.test(predict(fit_train, newdata = test_sample), test_sample$mpg)$p.value
```